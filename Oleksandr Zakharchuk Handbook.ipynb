{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4530361f-4fb3-4c56-a365-01daab7aac19",
   "metadata": {},
   "source": [
    "# 1. Import libraries:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456e1aeb-30b0-4b96-b13f-2bd5b2fccaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from pylab import rcParams\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44377d-c489-4b4d-923a-6dc5396b0bf1",
   "metadata": {},
   "source": [
    "# 2. Directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ededc891-c928-4766-977d-528ae90b36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['bar'].value_counts()       # get distinct element and their number of occurence\n",
    "#df.shape[0]                    # get dataframe row count\n",
    "#df.drop(columns=['bar'])       # remove column from dataframe\n",
    "#df.copy()                      # copy dataframe\n",
    "#np.ravel(df)                   # flatten array\n",
    "#np.unique(array)               # find the unique elements of numpy array\n",
    "#df['bar'].apply(pd.to_numeric) # convert to numeric type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c64310-e352-49f1-8e2c-cc4bc3cc4136",
   "metadata": {},
   "source": [
    "# 3. Basic functions:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6429b9a7-b20f-4f6a-800f-b719091d6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a value across the entire dataset. Output: {column: count, column: count, ...}\n",
    "def get_count_by_column(dataframe, seach, print_message=True):\n",
    "    unique, counts = np.unique(np.argwhere(np.isin(dataframe.to_numpy(), seach))[:, 1], return_counts=True)\n",
    "    result = dict(zip(dataframe.columns[unique], counts))\n",
    "    if print_message != True:\n",
    "        return result\n",
    "    print(f\"Search {seach} value per column: {dict() if not bool(result) else ''}\")\n",
    "    if bool(result):\n",
    "        for column, data in result.items():\n",
    "            print(f\"\\tColumn {column}: \", data)\n",
    "\n",
    "def get_match_mask(df, regex):\n",
    "    np_df = np.ravel(df.to_numpy())\n",
    "    mask = pd.Series(np_df).astype(str).str.contains(regex) #contains only works correctly with strings\n",
    "    return mask.values.reshape(df.shape) #return numpy.ndarray\n",
    "\n",
    "#returns a list of mask row positions where at least one value equals True\n",
    "def get_true_mask_row_positions(mask, invert_mask=False):\n",
    "    mask_series = np.any(~mask if invert_mask else mask, axis=1) #convert matrix to series; axis=1 is x coordinate\n",
    "    return np.where(mask_series == True) #returns the positions of elements from series\n",
    "\n",
    "def get_count_by_column_match(df, regex):\n",
    "    mask = get_match_mask(df, regex)\n",
    "    df_mask = pd.DataFrame(mask, columns=list(df.columns.values))\n",
    "    count_by_column = get_count_by_column(df_mask, True, False)\n",
    "    print(f\"Search count per column by regular expression '{regex}' (without single quotes): {dict() if not bool(count_by_column) else ''}\")\n",
    "    if bool(count_by_column):\n",
    "        for column, data in count_by_column.items():\n",
    "            print(f\"\\tColumn {column}: \", data)\n",
    "\n",
    "def get_unique_by_column_match(df, regex):\n",
    "    mask = get_match_mask(df, regex)\n",
    "    mdf = df.where(mask, other=np.nan) #mdf - masked dataframe\n",
    "    mdf = mdf.dropna(how='all') #drop rows where all values in row are nan\n",
    "    result = []\n",
    "    for column in mdf:\n",
    "        notna = mdf[column].dropna()\n",
    "        unique = pd.unique(notna)\n",
    "        if unique.size != 0:\n",
    "            result.append(f\"\\r\\n\\tColumn {column}: {list(unique)}\")\n",
    "    print(f\"Search unique value per column by regular expression '{regex}' (without single quotes): {''.join(result) if bool(result) else dict()}\")\n",
    "\n",
    "#regex parameter is for searching special characters by default\n",
    "def analysis_dataframe_values_by_column(df, values = [0, np.nan, None, [np.inf, -np.inf]], regex='\\W'):\n",
    "    for v in values:\n",
    "        get_count_by_column(df, v)\n",
    "    get_count_by_column_match(df, regex)\n",
    "    get_unique_by_column_match(df, regex)\n",
    "\n",
    "def show_heatmap_corr(df, fname = None):\n",
    "    corr_df = df.corr()\n",
    "    sns.set(font_scale = 0.8)\n",
    "    figure(figsize = (4, 3), dpi = 120)\n",
    "    sns.heatmap(corr_df)\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname)\n",
    "\n",
    "def std_scale(df, columns):\n",
    "    columns = columns if isinstance(columns, list) else [columns]\n",
    "    std_scale = preprocessing.StandardScaler().fit_transform(df[columns])\n",
    "    for idx, column in enumerate(columns):\n",
    "        print(f'Mean after standardization: {column} = {std_scale[:,idx].mean()}')\n",
    "        print(f'Standard deviation after standardization: {column} = {std_scale[:,idx].std()}\\n')\n",
    "    return std_scale\n",
    "\n",
    "def minmax_scale(df, columns):\n",
    "    columns = columns if isinstance(columns, list) else [columns]\n",
    "    minmax_scale = preprocessing.MinMaxScaler().fit_transform(df[columns])\n",
    "    for idx, column in enumerate(columns):\n",
    "        print(f'Min-value after min-max scaling: {column}={minmax_scale[:,idx].min()}')\n",
    "        print(f'Max-value after min-max scaling: {column}={minmax_scale[:,idx].max()}\\n')\n",
    "    return minmax_scale\n",
    "    \n",
    "def PCA_dimensionality_reduction(df, dimension, columns):\n",
    "    pca = PCA(n_components = dimension)\n",
    "    pca.fit(df)\n",
    "    df_PCA = pca.transform(df)\n",
    "    df_PCA = pd.DataFrame(df_PCA, columns=columns)\n",
    "    return df_PCA\n",
    "\n",
    "def get_KNeighborsClassifier_accuracy_score_list(X_train, y_train, X_test, y_test, k_range=range(1,100)):\n",
    "    accuracy_score_list = {}\n",
    "    for k in k_range:\n",
    "        cls = KNeighborsClassifier(n_neighbors=k)\n",
    "        cls.fit(X_train, y_train.values.ravel())\n",
    "        y_pred = cls.predict(X_test)\n",
    "        accuracy_score_list[k] = accuracy_score(y_test, y_pred)\n",
    "    return accuracy_score_list\n",
    "\n",
    "# 1. Sort by dictionary values (ascending)\n",
    "# 2. Get all the keys of the sorted dictionary\n",
    "# 3. Return the latest (best) key\n",
    "# Parameters:\n",
    "# accuracy_score_list type:dict\n",
    "def get_best_n_neighbors(accuracy_score_list):\n",
    "    return list(dict(sorted(accuracy_score_list.items(), key=lambda item: item[1])).keys())[-1]\n",
    "\n",
    "def get_training_and_test_parts(df, target):\n",
    "    X, y = df.loc[:, df.columns != target], df[target]\n",
    "    return train_test_split(X, y, random_state = 0)\n",
    "\n",
    "def get_value_counts_by_column(df_cr, column=False):\n",
    "    for column in df_cr[:] if column == False else df_cr[column]:\n",
    "        print('Column: ', column)\n",
    "        print(df_cr[column].value_counts(), '\\r\\n')\n",
    "        \n",
    "def get_features_target_split(df, target):\n",
    "    return df.loc[:, df.columns != target], df[target]\n",
    "\n",
    "def get_column_names_except_target(df, target):\n",
    "    return list(df.columns[df.columns != target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3d1b2-2b81-44a5-b112-497eff68b34f",
   "metadata": {},
   "source": [
    "# 4. Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9cccf-bdf1-4057-bf6e-644b7f96c608",
   "metadata": {},
   "source": [
    "## 4.1 Basic charts:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ccaaa8-a844-43b5-b009-ab3c804fd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_histogram(df, column):\n",
    "    fig = px.histogram(df[column], x=column)\n",
    "    fig.show()\n",
    "\n",
    "def show_histogram_for_multiple_columns(df, columns):\n",
    "    fig = go.Figure()\n",
    "    for column in columns:\n",
    "        fig.add_trace(go.Histogram(x=df[column], name=column))\n",
    "    fig.update_layout(barmode = 'overlay')\n",
    "    fig.show()\n",
    "    \n",
    "def show_pair_plot(df, x, y, kind='scatter'):\n",
    "    sns.jointplot(x=x, y=y, data=df, kind=kind);\n",
    "    \n",
    "def show_multiple_plot(df, columns, fname = None):\n",
    "    %config InlineBackend.figure_format = 'png'\n",
    "    df_pairplot = sns.pairplot(df[columns]);\n",
    "    if fname is not None:\n",
    "        df_pairplot.fig.savefig(\"fname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8feda-4a22-4d26-83f8-eff7cabb66ea",
   "metadata": {},
   "source": [
    "## 4.2 Building a classification graph that implements the voting of k nearest neighbors (KNeighborsClassifier):\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29613200-1b38-40af-9a5a-39ba8fb1734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_plot(X, y, n_neighbors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 41)\n",
    "\n",
    "    h           = .02  # step size in the mesh\n",
    "\n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "    cmap_bold  = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    rcParams['figure.figsize'] = 5, 5\n",
    "    for weights in ['uniform', 'distance']:\n",
    "        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        fig = plt.figure()\n",
    "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)   \n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "        plt.title(\"0/1 outcome classification (k = %i, weights = '%s')\" % (n_neighbors, weights))\n",
    "        plt.show()\n",
    "        fig.savefig(weights +'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bc709-d591-4297-8fee-fb28b056c3c2",
   "metadata": {},
   "source": [
    "## 4.3 Building decision boundaries for decision trees (DecisionTreeClassifier, DecisionBoundaryDisplay):\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b334b413-458a-449e-b579-678abdda8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_series_for_pair_train(feature_number, start=0):\n",
    "    series = []\n",
    "    chunk_number = feature_number\n",
    "    for n in range(start, feature_number):\n",
    "        for c in range(start, chunk_number):\n",
    "            series.extend([n])\n",
    "        chunk_number -= 1\n",
    "    return series\n",
    "\n",
    "def get_second_series_for_pair_train(feature_number, start=0):\n",
    "    series = []\n",
    "    chunk_number = feature_number\n",
    "    shift = start\n",
    "    for n in range(start, feature_number):\n",
    "        shift += 1\n",
    "        for c in range(start, chunk_number):\n",
    "            series.extend([c + shift])\n",
    "        chunk_number -= 1\n",
    "    return series\n",
    "\n",
    "#plot_colors=\"ryb\" is a possible value\n",
    "def show_decision_trees_plot(data, y, feature_number, n_classes, plot_colors, plot_column_count=3, plot_step=0.02, start=0):\n",
    "    first_series = get_first_series_for_pair_train(feature_number, start)\n",
    "    second_series = get_second_series_for_pair_train(feature_number, start)\n",
    "    feature_pairs = [list(a) for a in zip(first_series, second_series)]\n",
    "    for pairidx, pair in enumerate(feature_pairs):\n",
    "        # We only take the two corresponding features\n",
    "        X = data.iloc[:, pair]\n",
    "\n",
    "        # Train\n",
    "        clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "        # Plot the decision boundary\n",
    "        plot_row_count = len(feature_pairs) // plot_column_count + 1\n",
    "        ax = plt.subplot(plot_row_count, plot_column_count, pairidx + 1)\n",
    "        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            clf,\n",
    "            X,\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            response_method=\"predict\",\n",
    "            ax=ax,\n",
    "            xlabel=data.columns.values[pair[0]],\n",
    "            ylabel=data.columns.values[pair[1]],\n",
    "        )\n",
    "\n",
    "        # Plot the training points\n",
    "        for i, color in zip(range(n_classes), plot_colors):\n",
    "            idx = np.where(y == i)[0]\n",
    "            plt.scatter(\n",
    "                X.iloc[idx, 0],\n",
    "                X.iloc[idx, 1],\n",
    "                c=color,\n",
    "                label=np.unique(y)[i],\n",
    "                # cmap=plt.cm.RdYlBu,\n",
    "                edgecolor=\"black\",\n",
    "                s=15,\n",
    "            )\n",
    "\n",
    "    # plt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\n",
    "    plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n",
    "    _ = plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a0ed2-b962-4cdb-9109-bc5dd69a4632",
   "metadata": {},
   "source": [
    "## 4.4 Visualizing K-Means Clustering Results on Data Reduced with PCA (KMeans, PCA)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88277708-15d3-4e5d-9351-a8b0dba4e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# reduced_data shape:(n, 2)\n",
    "def show_KMeans_plot(reduced_data, n_clusters, n_init=4):\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=n_clusters, n_init=n_init)\n",
    "    kmeans.fit(reduced_data)\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(\n",
    "        Z,\n",
    "        interpolation=\"nearest\",\n",
    "        extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "        cmap=plt.cm.Paired,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(\n",
    "        centroids[:, 0],\n",
    "        centroids[:, 1],\n",
    "        marker=\"x\",\n",
    "        s=169,\n",
    "        linewidths=3,\n",
    "        color=\"w\",\n",
    "        zorder=10,\n",
    "    )\n",
    "    plt.title(\n",
    "        \"K-means clustering on the digits dataset (PCA-reduced X)\\n\"\n",
    "        \"Centroids are marked with white cross\"\n",
    "    )\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035239ae-5cf5-47e9-ae89-317d7881e830",
   "metadata": {},
   "source": [
    "## 4.5 HDBSCAN: visualization\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffc2afa-a388-4ba8-aa92-4099a96df23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_HDBSCAN_plot(X):\n",
    "    reduced_data = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=15).fit(reduced_data)\n",
    "    color_palette = sns.color_palette('deep', 8)\n",
    "    cluster_colors = [color_palette[x] if x >= 0\n",
    "                      else (0.5, 0.5, 0.5)\n",
    "                      for x in clusterer.labels_]\n",
    "    cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                             zip(cluster_colors, clusterer.probabilities_)]\n",
    "    plt.scatter(*reduced_data.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db546edc-06db-428e-a030-d381120b98d3",
   "metadata": {},
   "source": [
    "## 4.6 Construction of a hierarchical cluster dendrogram\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5cbbf1-44cc-45d1-9ec8-8bb87abdb206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba16f0-7e58-4be1-ac9b-a03fc0a5eda7",
   "metadata": {},
   "source": [
    "## 4.7 Building a graph for selecting the optimal number of clusters (kmeans, kmeans.inertia_, silhouette_score)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74bf3517-2e00-4d25-8fdd-b963171f34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# metric possible values:silhouette_score, squared_distances_sum\n",
    "def kmeans_metric_plot(X, clusters_range, metric):\n",
    "    result = []\n",
    "    for c in clusters_range :\n",
    "        kmeans = KMeans(n_clusters=c)\n",
    "        kmeans.fit(X)\n",
    "        if metric == 'squared_distances_sum':\n",
    "            result.append(kmeans.inertia_)\n",
    "        elif metric == 'silhouette_score':\n",
    "            cluster_labels = kmeans.labels_\n",
    "            result.append(silhouette_score(X, cluster_labels))\n",
    "    plt.plot(clusters_range, result, 'bx-')\n",
    "    plt.xlabel('Values of K') \n",
    "    if metric == 'squared_distances_sum':\n",
    "        plt.ylabel('Sum of squared distances/Inertia') \n",
    "        plt.title('Elbow Method For Optimal K')\n",
    "    elif metric == 'silhouette_score':\n",
    "        plt.ylabel('Silhouette score') \n",
    "        plt.title('Silhouette analysis For Optimal k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afebdcf-2f31-4e0e-9bd2-962ed3b8b0e4",
   "metadata": {},
   "source": [
    "## 4.8 Building a graph of information loss with a decrease in dimension:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5169be76-734d-4a81-a6ca-b4f7aeba5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_explained_variance_ratio_plot(PCA):\n",
    "    plt.bar(range(len(PCA.explained_variance_ratio_)), PCA.explained_variance_ratio_, alpha=0.5, align='center', label='Individual explained variance')\n",
    "    plt.step(range(len(PCA.explained_variance_ratio_.cumsum())), PCA.explained_variance_ratio_.cumsum(), where='mid',label='Cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal component index')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print('PCA.explained_variance_               ', PCA.explained_variance_)\n",
    "    print('PCA.explained_variance_ratio_         ', PCA.explained_variance_ratio_)\n",
    "    print('PCA.explained_variance_ratio_.cumsum()', PCA.explained_variance_ratio_.cumsum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
