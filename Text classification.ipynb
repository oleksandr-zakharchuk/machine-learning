{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c010a4d5-ed0d-420d-ab67-e789a122da24",
   "metadata": {},
   "source": [
    "# 1. Primary data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18797cd1-5f28-4a88-a3e0-45c17bb98d2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Connecting libraries and importing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09af1c46-f4d0-4145-8435-59748257d611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run \"../../Oleksandr Zakharchuk Handbook.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3483122-09e8-4aed-b2e6-c6098cd6c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33437e92-6930-42d9-827d-402e3398ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760079f9-cadc-4c37-a42e-e6aa476aa9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a17737-38d9-43fa-bd4c-1de0d3bc7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62af7ed-d8c6-4cd0-83b0-05dd3302b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.test.gensim_fixt import setup_module\n",
    "import gensim\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cf47cd-1f10-4898-8433-4445bdff429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965d8c25-a211-44d0-b2ca-b777525316e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data Folder/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780afb53-e3e3-4d81-8ca8-da91750bc597",
   "metadata": {},
   "source": [
    "For speed and convenience, we can temporarily take a limited number of lines:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff8c5d2-ff70-40a8-90e4-71e27e14a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[0:999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d76bb2-5c58-4ffb-9f62-f90f71ca83a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 General information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114e50d5-c46b-4c75-b0a0-5c31a3fbdfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     1000 non-null   object\n",
      " 1   sentiment  1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e42c56d-2e0d-4b04-a3f7-8ab09e5bd83c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Nothing is sacred. Just ask Ernie Fosselius. T...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I hated it. I hate self-aware pretentious inan...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I usually try to be professional and construct...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>If you like me is going to see this in a film ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>This is like a zoology textbook, given that it...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    One of the other reviewers has mentioned that ...  positive\n",
       "1    A wonderful little production. <br /><br />The...  positive\n",
       "2    I thought this was a wonderful way to spend ti...  positive\n",
       "3    Basically there's a family where a little boy ...  negative\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "995  Nothing is sacred. Just ask Ernie Fosselius. T...  positive\n",
       "996  I hated it. I hate self-aware pretentious inan...  negative\n",
       "997  I usually try to be professional and construct...  negative\n",
       "998  If you like me is going to see this in a film ...  negative\n",
       "999  This is like a zoology textbook, given that it...  negative\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec40557-cdb1-4c86-9a35-e83636233aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'sentiment'\n",
    "target = [target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94506d-4290-4930-9c4e-7b732b9a1b05",
   "metadata": {},
   "source": [
    "# 2. Type conversion and value adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fb5a0-5986-49d5-9aca-4d252474d000",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Parsing Data Types and Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979a58c4-2683-42bc-89d6-109cf179ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.replace('<br />', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a26d9983-e8d1-4a58-ad4a-35ea6ae75eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 0 value per column: {}\n",
      "Search nan value per column: {}\n",
      "Search None value per column: {}\n",
      "Search [inf, -inf] value per column: {}\n",
      "Search count per column by regular expression '<br' (without single quotes): {}\n",
      "Search unique value per column by regular expression '<br' (without single quotes): {}\n"
     ]
    }
   ],
   "source": [
    "analysis_dataframe_values_by_column(df, regex='<br')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c8691-b4d1-45da-9bb4-d8aaa46612a0",
   "metadata": {},
   "source": [
    "All <br \\/> tags have been removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde8441-6f5f-4486-a7ac-5e2e19a30187",
   "metadata": {},
   "source": [
    "# 4. Dividing the dataset into training and test parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1d810d-a584-4b3e-a911-0c05bec6edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_features_target_split(df, target_name)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d26609da-b751-49b7-82c5-d489622e9249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b4f1a-36f0-453d-a279-907b9cc845cc",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd73e91-6603-418b-9f2a-4b36b2aa3620",
   "metadata": {},
   "source": [
    "## 3.1 Bag Of Word (BOW) (CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b85c0-0fb0-4724-984d-d426951f8a08",
   "metadata": {},
   "source": [
    "Thus, a corpus of documents can be represented by a matrix with one row per document and one column per token (eg word) occurring in the corpus.\n",
    "\n",
    "We call vectorization the general process of converting a set of text documents into vectors of numerical features. This particular strategy (tokenization, counting and normalization) is called the Bag of Words or Bag of n-grams representation. Documents are described by occurrences of words, completely ignoring information about the relative position of words in the document.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb488c4b-dcee-45da-b284-3dc72ebf9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X_train['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778cac2-24f6-4fc2-af90-c5c3d0533217",
   "metadata": {},
   "source": [
    "Get a list of words (features) used in the review:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0feac071-4fb1-4a1d-9f4f-79f61e19642b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '007', ..., 'zulu', 'zzzzzzzzzzzzzzzzzz', 'ísnt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_get_feature_names_out = vectorizer.get_feature_names_out()\n",
    "vectorizer_get_feature_names_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97689daf-5542-476a-b0c8-58b862cc3dea",
   "metadata": {},
   "source": [
    "The number of unique words in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f68fcad6-8bed-4702-a996-08e92c1c37b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14723"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c23b0-17eb-4e67-bc8d-77034a2c54a9",
   "metadata": {},
   "source": [
    "Let's get our matrix, which shows the number of words in each review:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc28e32-4cb7-45ae-9a2d-eb056c30dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14f5d52c-ecce-4264-9c33-319f042356b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 14723)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aea402-f8c4-4cdd-aded-813439e25460",
   "metadata": {},
   "source": [
    "As we can see, the number of columns is equal to the number of unique words in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f633f69-addd-4e27-ba77-e219c6cfab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 3))  #‘word’, ‘char’, ‘char_wb’\n",
    "X2 = vectorizer2.fit_transform(X_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03d3ba9-c009-45a8-893b-e7ebaa11d5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('must', 127879),\n",
       " ('admit', 4183),\n",
       " ('was', 218048),\n",
       " ('expecting', 62430),\n",
       " ('something', 174550),\n",
       " ('quite', 154955),\n",
       " ('different', 51336),\n",
       " ('from', 72610),\n",
       " ('my', 128055),\n",
       " ('first', 68434),\n",
       " ('viewing', 216486),\n",
       " ('of', 134163),\n",
       " ('cut', 47220),\n",
       " ('last', 109834),\n",
       " ('night', 130674),\n",
       " ('though', 202505),\n",
       " ('delighted', 49362),\n",
       " ('with', 227204),\n",
       " ('the', 186651),\n",
       " ('unexpected', 212706),\n",
       " ('australian', 22544),\n",
       " ('horror', 89812),\n",
       " ('gem', 74808),\n",
       " ('am', 7955),\n",
       " ('true', 210676),\n",
       " ('fan', 64004),\n",
       " ('as', 19279),\n",
       " ('they', 199055),\n",
       " ('come', 42096),\n",
       " ('and', 9544),\n",
       " ('found', 71739),\n",
       " ('to', 204462),\n",
       " ('not', 131824),\n",
       " ('only', 141672),\n",
       " ('be', 24617),\n",
       " ('best', 28145),\n",
       " ('genre', 74997),\n",
       " ('australia', 22527),\n",
       " ('has', 81726),\n",
       " ('ever', 60797),\n",
       " ('produced', 153172),\n",
       " ('but', 33110),\n",
       " ('one', 140908),\n",
       " ('great', 78812),\n",
       " ('parody', 146283),\n",
       " ('comedy', 42259),\n",
       " ('films', 67494),\n",
       " ('late', 109965),\n",
       " ('concern', 43460),\n",
       " ('is', 98251)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer2.vocabulary_.items())[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacf8ce-64d8-44e7-a1bd-f96b6d5b4708",
   "metadata": {},
   "source": [
    "Let's get our words (features), as well as the sequence of links for 2-3 words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cc067ce-bfb4-45c5-8deb-3d0725d2a4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '00 am', '00 am stayed', ..., 'ísnt', 'ísnt entertaining',\n",
       "       'ísnt entertaining if'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752f796-35db-4ab6-b090-95845a4ff2c2",
   "metadata": {},
   "source": [
    "The number of unique words in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "482e6c34-c5b2-4d52-958e-5cbb56bb1036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234260"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae72008-8156-44da-aa54-7d050f246eca",
   "metadata": {},
   "source": [
    "Matrix of occurring words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19b8b2a2-5b68-48ef-b40b-2294f4583ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "987e6ad2-6154-4f15-a679-db57127a4ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 234260)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d287a7d-1487-42a6-97d8-9501b4dd6218",
   "metadata": {},
   "source": [
    "## 3.2 TF_IDF (CountVectorizer, TfidfTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087ba86-73a7-4939-a816-1c8b07a0acf8",
   "metadata": {},
   "source": [
    "Prepare the pipeline and train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8f79230-3eab-455b-95c5-1df8f106229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('count', CountVectorizer(vocabulary=vectorizer_get_feature_names_out)), \n",
    "        ('tfid', TfidfTransformer())\n",
    "    ]\n",
    ").fit(X_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c66e065-803b-4432-a228-81671dfc56ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(vocabulary=array([&#x27;00&#x27;, &#x27;000&#x27;, &#x27;007&#x27;, ..., &#x27;zulu&#x27;, &#x27;zzzzzzzzzzzzzzzzzz&#x27;, &#x27;ísnt&#x27;],\n",
       "      dtype=object))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(vocabulary=array([&#x27;00&#x27;, &#x27;000&#x27;, &#x27;007&#x27;, ..., &#x27;zulu&#x27;, &#x27;zzzzzzzzzzzzzzzzzz&#x27;, &#x27;ísnt&#x27;],\n",
       "      dtype=object))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(vocabulary=array(['00', '000', '007', ..., 'zulu', 'zzzzzzzzzzzzzzzzzz', 'ísnt'],\n",
       "      dtype=object))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70fcaa31-243a-43a1-b04c-126385a8180f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['count'].transform(X_train['review']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e5c23-c662-4c8c-9e77-a7fe9b94d0bc",
   "metadata": {},
   "source": [
    "We see the words (features):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f9e0267-93e6-482a-9b22-9cd1e8ba5fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '007', ..., 'zulu', 'zzzzzzzzzzzzzzzzzz', 'ísnt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['count'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea330a9-a436-4e25-b9b1-0ed61a9821a6",
   "metadata": {},
   "source": [
    "Number of words (features):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3855b8c8-3baf-46eb-aa59-3771be297802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14723"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe['count'].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c9ab4-f13d-47e9-b5fd-85ba3d5a9c7b",
   "metadata": {},
   "source": [
    "Next, you need to highlight those words that are most often found in the current sentence, but not found in other sentences. To do this, let's look at the tfid metric (tf is the frequency of the word, idf is the inverse frequency in the document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db0e0bd3-e4bd-469d-b817-48c91b5e5249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.89933122, 5.4293276 , 6.81562196, ..., 6.81562196, 6.81562196,\n",
       "       6.81562196])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['tfid'].idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5085d61-6e60-437b-8f63-5b20e2943c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X = vectorizer_tfidf.fit_transform(X_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8919cdca-1f98-4709-a2e2-7a970a46c9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '007', ..., 'zulu', 'zzzzzzzzzzzzzzzzzz', 'ísnt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daee77fc-fba5-4784-86db-8264c98bdb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5597)\t0.039589484285865825\n",
      "  (0, 7831)\t0.05804152816229731\n",
      "  (0, 4349)\t0.04070505939805111\n",
      "  (0, 14054)\t0.02983697400209609\n",
      "  (0, 13309)\t0.08661010119341983\n",
      "  (0, 2322)\t0.08393337600498052\n",
      "  (0, 13419)\t0.1000624790968887\n",
      "  (0, 11351)\t0.07798060395217861\n",
      "  (0, 1283)\t0.052805061211738454\n",
      "  (0, 14633)\t0.04401333754281163\n",
      "  (0, 9194)\t0.027830554894485455\n",
      "  (0, 3011)\t0.055718587377774006\n",
      "  (0, 10163)\t0.1000624790968887\n",
      "  (0, 8645)\t0.021877782841683537\n",
      "  (0, 13212)\t0.03717367840531482\n",
      "  (0, 551)\t0.048305667164118694\n",
      "  (0, 13063)\t0.1000624790968887\n",
      "  (0, 7107)\t0.1000624790968887\n",
      "  (0, 11119)\t0.04918082390135742\n",
      "  (0, 1771)\t0.06131757502665992\n",
      "  (0, 2146)\t0.08393337600498052\n",
      "  (0, 1660)\t0.1000624790968887\n",
      "  (0, 11902)\t0.08393337600498052\n",
      "  (0, 14330)\t0.06475835065848883\n",
      "  (0, 4818)\t0.06240557953482789\n",
      "  :\t:\n",
      "  (669, 8645)\t0.02906555331052809\n",
      "  (669, 13212)\t0.09877358590281722\n",
      "  (669, 14342)\t0.13296372385871721\n",
      "  (669, 8033)\t0.05007124141908814\n",
      "  (669, 13194)\t0.03613801386372284\n",
      "  (669, 13225)\t0.021363783928129636\n",
      "  (669, 980)\t0.03287966071877815\n",
      "  (669, 8187)\t0.04216273374725582\n",
      "  (669, 7922)\t0.04616987323649668\n",
      "  (669, 6964)\t0.06584399214108184\n",
      "  (669, 6598)\t0.021947997380360615\n",
      "  (669, 9145)\t0.05600698461785734\n",
      "  (669, 9147)\t0.03101016738319135\n",
      "  (669, 4553)\t0.052281692477061825\n",
      "  (669, 1391)\t0.05275551609766423\n",
      "  (669, 9153)\t0.041344231282110386\n",
      "  (669, 13348)\t0.10413822331967706\n",
      "  (669, 660)\t0.04018996927419269\n",
      "  (669, 899)\t0.028596665830545837\n",
      "  (669, 599)\t0.061480242436284104\n",
      "  (669, 13168)\t0.23545680849650052\n",
      "  (669, 14484)\t0.026283896545643937\n",
      "  (669, 8719)\t0.04107885603400222\n",
      "  (669, 5316)\t0.06693086773426385\n",
      "  (669, 14259)\t0.08428075149302962\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcd9c9-7e76-4111-a47e-f6eb1ee98c2e",
   "metadata": {},
   "source": [
    "## 3.3 NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15a6e5-30df-439d-b827-38c2970c4a39",
   "metadata": {},
   "source": [
    "### 3.3.1 nltk.stem, nltk.stem.porter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b735f05-69f9-4367-aa5b-c4bbce50245b",
   "metadata": {},
   "source": [
    "The PorterStemmer function in the NLTK (Natural Language Toolkit) library performs word stemming based on the Porter algorithm. Stemming is the process of reducing word forms to their stems or roots (called stems) by removing endings and affixes. For example, if you apply the PorterStemmer function to the word \"running\", it will return the stem \"run\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff3f772b-ba35-4ace-9b69-b773ffe5e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08e6a1-1ca7-4f7f-903d-52b0115025f8",
   "metadata": {},
   "source": [
    "Let's bring our words (features) to the basics of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46dd885f-3d58-4072-9c8b-6d9b58cb7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = [stemmer.stem(v) for v in vectorizer_get_feature_names_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16ea3ed6-7e07-436c-8d69-69c209259d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 000 007 00am 10 100 1000 100th 101 102 103 105 10i 10thi 11 12 120 13 135 13th 14 15 16 17 1700 177 1794 18 1800 1840 1860 18th 19 1900 1903 1919 1920 1920 1921 1922 1923 1928 1929 1930 1930 1932 1934 1936 1937 1938 1939 1940 1940 1941 1944 1945 1947 1948 1949 1950 1950 1951 1952 1953 1954 1955 1956 1959 1960 1960 1963 1964 1966 1968 1969 1970 1971 1972 1973 1974 1976 1977 1980 1980 1981 1982 1983 1984 1984ish 1985 1987 1990 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 19th 1m 1sound 1st 20 2000 20000 2000 2001 2002 2003 2004 2005 2006 2008 2012 2036 20 20th 21 22 23 24 25 250 2500 25min 26 28 29 2am 2ftm 2hour 2hr 2nd 30 300 3000 30pm 30 31 31st 32 33 35 35c 36 360 36th 37 38 3rd 40 400 40 43 44c 45 450 47 48 49th 4th 50 500 50c 50 52 53 55 5539 56 57 58 588 5th 5yo 60 60 63 64 666 69 70 700 70 74 747 75 75c 77 78 80 8000 80 85 87 88 89 90 90c 90 95 950 99 9th _inspire_ aaliyah aamir aaron abandon abandon abandon abbey abbot abbott abbrevi abducte abed abet abhorr abid abid abil abil abject abl abli abo aboard abod abomin abort abound about abov abraham abruptli absenc absent absolut absolut absorb absorb abstract absurd absurd abund abund abus abus abus abuzz abysm academi acceler accent accent accentu accentu accept accept accept accept accept accept access access accid accident accident acclaim acclaim accolad accompani accompani accompani accomplish accomplish accomplish accomplish accord account account account accuraci accur accur accus accus accustom ace ach achiev achiev achiev ach achingli acid acin acknowledg acknowledg acknowledg acquaint acquir acquir across act act act action action activ activ activ activ actor actor actress actress act actual actual actual ad adam adam adapt adapt adapt adapt add ad addendum addict addict addict addict ad addit addit addit address address add adelaid adequ adequ adher adher adjac adject adjust adjust administr admir admir admir admir admir admiss admit admit admittedli adoby adolesc adoor adopt adopt adopt adopt ador ador ador adrenalin adrian adrien adrift adroit ad adult adulteri adulthood adult advanc advanc advanc advanc advantag advantag advent adventur adventur adventur adversari adversari advers advertis advert advic advis advis advis advisor advoc aesthet affair affair affect affect affect affect affection affect affect affin affirm affleck afflict afflict afford afghan afghanistan afloat afoot aforement afraid africa african after afterl aftermath afternoon afternoon afterward afterward afterword ag again against agatha age age age agenc agenc agenda agent agent age aggh aggrav aggress aghast agil age agn ago agon agoni agre agre agre agreement agre aguayo ah aha ahab ahalf ahead aheheh ahem ahna aid aidan aid aiden aid aid ail aim aim aim aimless aim ain air aircraft air airhead air airplan airship airwav ajay aka akasha akin aksar al alabama alain alan alarm ala alaska albeit albeniz albert alberto albino albino album alchemi alcohol alcohol alcohol aldridg alec aleck alecki alert alert alex alexand alexandra alexandr alfr alfr alger algier algonquin ali alia alias alibi alic alicia alien alien alien alien align alik alin alistair aliti aliv all alleg allegi allegor allegori allegori allen alley allianc allig allison allot allow allow allow allow allthough alltim allud allus almighti almodóvar almost alon along alongsid alot alpha alpo alp alreadi alright also alt alter altern altern altern altern although altman altogeth altro altruist alumni alvarado alvin alway am amanda amass amateur amateurish amateurishli amateurish amateur amatur amaz amaz amaz amaz amazingli ambassador amber ambianc ambigu ambigu ambigu ambit ambit ambiti america american american america amiabl amidst amitabh amithab ammo ammunit amok among amongst amor amount amount amour amp ampl amplifi amput amtrak amus amus amus amus ami an ana anaheim analyst analyz anastasia anatomi ancestor anchor anchorman ancient and anderson and andi andr andrew andrew android andromeda andrzej and andi anesthesia aneur ang angel angela angel angelina angeliqu angel anger anger anger anglai angl angl angl angri angst animagin anim anim anim anim anim anim anim anim ankur anna annakin annan ann annett anni annik announc announc annoy annoyingli annoy annul anonym anorex anoth an answer answer answer antagonist antal antarct antarctica antartica antedot anthem anthem antholog anthoni anthropologist anti antibodi anticip anticip anticip anticip antic antonioni ant antti anu anxieti anxiou anxious ani anybodi anyhow anymor anyon anyth anyway anyway anywher aot apanowicz apart apartheid apart apart apathi ape apertur apex aplomb apocalypt apolig apollo apolog apolog appal appal appallingli appar appar appeal appeal appeal appear appear appear appear appear appear applaud applaud applaus appl applic applic appli appli appreci appreci appreci appreci apprehend apprentic approach approach approach approach appropri appropri appropri approv approxim apropo apt arab arabia arab araki arbitrarili \n"
     ]
    }
   ],
   "source": [
    "print(' '.join(singles)[0:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b254e-10be-45a4-8b11-4c18173cf15f",
   "metadata": {},
   "source": [
    "### 3.3.2 PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853d50e-9c68-4050-9a41-d23751008e97",
   "metadata": {},
   "source": [
    "The PunktSentenceTokenizer function in the NLTK (Natural Language Toolkit) library is used to split text into sentences using a trained Punkt tokenizer model.\n",
    "\n",
    "PunktSentenceTokenizer implements an unsupervised learning algorithm that parses text and finds punctuation patterns used to separate sentences. It relies on a set of rules that are applied to text to determine the most likely places where sentences end.\n",
    "\n",
    "The workflow of the PunktSentenceTokenizer function is as follows:\n",
    "\n",
    "Training: To train a PunktSentenceTokenizer model, you need to use a training corpus containing sentence-separated texts. NLTK provides some pre-trained models that can be used out of the box. However, if you have your own training corpus, you can train the PunktSentenceTokenizer model based on it.\n",
    "\n",
    "Tokenization: Once the PunktSentenceTokenizer model has been trained, you can use it to tokenize text into sentences. The PunktSentenceTokenizer function takes input text and returns a list of sentences separated into separate lines.\n",
    "\n",
    "Slicing Algorithm: PunktSentenceTokenizer parses text by applying a set of rules to determine where each sentence ends. It takes into account various factors, such as the position of punctuation, the use of abbreviations, and also takes into account the contextual features of the text.\n",
    "\n",
    "All in all, the PunktSentenceTokenizer function provides a reasonably reliable and efficient way to split text into sentences. However, it is worth noting that it is not perfect and may have some limitations or bugs in some cases, especially when processing text with non-standard features or complex sentence structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bf49d40-b5dd-4547-a92e-5e9dfd7d8318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked.\",\n",
       " 'They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO.',\n",
       " 'Trust me, this is not a show for the faint hearted or timid.',\n",
       " 'This show pulls no punches with regards to drugs, sex or violence.',\n",
       " 'Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary.',\n",
       " 'It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda.',\n",
       " \"Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare.\",\n",
       " \"Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around.\",\n",
       " \"The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence.\",\n",
       " \"Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PST = PunktSentenceTokenizer()\n",
    "tokenized = PST.tokenize(X_train['review'][0])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee310e0-0805-4273-a2ab-c3648146a524",
   "metadata": {},
   "source": [
    "## 3.4 LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6960bf10-c4d7-4bdf-be11-6768bb3960a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476db85-3211-47e3-aaaf-db7a266d7ede",
   "metadata": {},
   "source": [
    "Let's transform into vectors of numerical features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f11d66fd-f278-48e4-9f8c-f0246fd996c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_lr = CountVectorizer(analyzer='word', ngram_range=(1, 3))  #‘word’, ‘char’, ‘char_wb’\n",
    "X_lr = vectorizer_lr.fit_transform(df_lr['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a43c76-dfb4-452c-b0fd-c3f2d034cc37",
   "metadata": {},
   "source": [
    "I split the dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09acdf60-6472-4fcb-adfd-b27f87eb6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr.toarray(), df_lr['sentiment'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e9d80-8907-4e43-bf6f-c181033cc252",
   "metadata": {},
   "source": [
    "Let's train:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0bdac1a-5dde-4475-b873-20c0c1adcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71266897-c871-48f0-88ea-05228e7648a4",
   "metadata": {},
   "source": [
    "Let's make a prediction for the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8efb091-9d5c-4e2d-8389-5b888546d3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'negative', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'negative', 'positive', 'positive', 'negative',\n",
       "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'negative', 'negative', 'positive', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'negative', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'negative', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'negative', 'negative', 'positive',\n",
       "       'negative', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'positive', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'positive', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'positive', 'negative', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'positive', 'positive', 'negative', 'negative', 'positive',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'negative'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5dd71-eea6-44c0-a357-689649218bd8",
   "metadata": {},
   "source": [
    "## 3.5 spyCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ff583-aaf8-424d-8e64-623da824d0d8",
   "metadata": {},
   "source": [
    "spaCy NLP (Natural Language Processing) is a library for Natural Language Processing in Python. It provides tools for performing various text processing tasks such as tokenization, lemmatization, part-of-word markup, named entity extraction, parsing, and more.\n",
    "\n",
    "The main features of the spaCy library are:\n",
    "\n",
    "Tokenization: Dividing text into individual words or tokens. SpaCy provides efficient methods for language-specific tokenization.\n",
    "\n",
    "Lemmatization: Reducing words to their base or lemmatic forms. This is useful, for example, for matching different forms of the same word.\n",
    "\n",
    "Part-of-speech markup: Identify parts of speech for each token in a text, such as nouns, verbs, adjectives, etc.\n",
    "\n",
    "Named Entity Extraction (NER): Recognition and classification of named entities in text, such as the names of people, organizations, locations, etc.\n",
    "\n",
    "Parsing: Analysis of the structure of sentences to determine relationships between words, such as dependencies and syntactic relationships.\n",
    "\n",
    "Vector word representations: spaCy provides pre-trained models for creating word vector representations that can be used for text comparison and semantic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669e300-0198-4621-8e45-865a4d70df83",
   "metadata": {},
   "source": [
    "Loading a pretrained NER model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad3dded5-21f9-4a7a-a62a-14e332a65913",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57f411e7-3321-4344-9b69-174bf96f8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(X_train['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0617d6a2-dd55-4a94-a203-ff9c422ba841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One one NUM CD nsubj Xxx True True\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "other other ADJ JJ amod xxxx True True\n",
      "reviewers reviewer NOUN NNS pobj xxxx True False\n",
      "has have AUX VBZ aux xxx True True\n",
      "mentioned mention VERB VBN ROOT xxxx True False\n",
      "that that SCONJ IN mark xxxx True True\n",
      "after after ADP IN prep xxxx True True\n",
      "watching watch VERB VBG pcomp xxxx True False\n",
      "just just ADV RB advmod xxxx True True\n",
      "1 1 NUM CD nummod d False False\n",
      "Oz oz NOUN NN compound Xx True False\n",
      "episode episode NOUN NN dobj xxxx True False\n",
      "you you PRON PRP nsubjpass xxx True True\n",
      "'ll will AUX MD aux 'xx False True\n",
      "be be AUX VB auxpass xx True True\n",
      "hooked hook VERB VBN ccomp xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "They they PRON PRP nsubj Xxxx True True\n"
     ]
    }
   ],
   "source": [
    "for token in doc[0:20]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e2049-82a3-48ce-bbb6-0dcfe29e5a90",
   "metadata": {},
   "source": [
    "Visualization of the proposed syntactic structure of a sentence using spaCy. Arrows point from child elements to head elements and are labeled with their relationship types (must be uncommented and run to view):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788ba6d-1b1d-4cbd-9924-4c0f6aba6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\spacy\\displacy\\__init__.py:108: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d3e81a2cf9ea48eb9f1cf090fa76bbbb-0\" class=\"displacy\" width=\"3375\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">One</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">other</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">reviewers</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">mentioned</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">after</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">watching</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">just</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">Oz</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">episode</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">you</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">'ll</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">be</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">hooked.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">They</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,177.0 1090.0,177.0 1090.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-1\" stroke-width=\"2px\" d=\"M70,527.0 C70,439.5 200.0,439.5 200.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M200.0,529.0 L208.0,517.0 192.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-2\" stroke-width=\"2px\" d=\"M420,527.0 C420,352.0 730.0,352.0 730.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,529.0 L412,517.0 428,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-3\" stroke-width=\"2px\" d=\"M595,527.0 C595,439.5 725.0,439.5 725.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,529.0 L587,517.0 603,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-4\" stroke-width=\"2px\" d=\"M245,527.0 C245,264.5 735.0,264.5 735.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,529.0 L743.0,517.0 727.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-5\" stroke-width=\"2px\" d=\"M945,527.0 C945,439.5 1075.0,439.5 1075.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,529.0 L937,517.0 953,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-6\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,89.5 3020.0,89.5 3020.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,529.0 L1287,517.0 1303,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-7\" stroke-width=\"2px\" d=\"M1470,527.0 C1470,177.0 3015.0,177.0 3015.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,529.0 L1462,517.0 1478,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-8\" stroke-width=\"2px\" d=\"M1470,527.0 C1470,439.5 1600.0,439.5 1600.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1600.0,529.0 L1608.0,517.0 1592.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-9\" stroke-width=\"2px\" d=\"M1820,527.0 C1820,439.5 1950.0,439.5 1950.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,529.0 L1812,517.0 1828,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-10\" stroke-width=\"2px\" d=\"M1995,527.0 C1995,352.0 2305.0,352.0 2305.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,529.0 L1987,517.0 2003,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-11\" stroke-width=\"2px\" d=\"M2170,527.0 C2170,439.5 2300.0,439.5 2300.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,529.0 L2162,517.0 2178,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-12\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,264.5 2310.0,264.5 2310.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310.0,529.0 L2318.0,517.0 2302.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-13\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,264.5 3010.0,264.5 3010.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-14\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,352.0 3005.0,352.0 3005.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,529.0 L2687,517.0 2703,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-15\" stroke-width=\"2px\" d=\"M2870,527.0 C2870,439.5 3000.0,439.5 3000.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,529.0 L2862,517.0 2878,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-16\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,2.0 3025.0,2.0 3025.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d3e81a2cf9ea48eb9f1cf090fa76bbbb-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3025.0,529.0 L3033.0,517.0 3017.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(doc[0:20], style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6715bc68-d9ab-4c20-a432-078f8957ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2e808ad-7698-4db8-ba2b-dd09598f83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b53e3aff-910e-49d3-99e1-66cc6d90cb6e",
   "metadata": {},
   "source": [
    "Let's define a pattern that will be matched against the text, and the Matcher will find all sections of the text where this sequence of tokens occurs in the specified order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ba00819-5ae4-4945-bf68-1eaaf4e8a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"senses\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"particularly\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c9c1cf6-e0b1-445f-8746-bc91ecb48d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"my_matcher\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bffcc5ac-370b-4073-ba7b-a534e368e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(X_train['review'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec38ea64-79d2-4055-9256-8514a29fa9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf7514-2cfb-40f0-865c-2ed51cd346c5",
   "metadata": {},
   "source": [
    "We get:  \n",
    "match identifier (match_id),  \n",
    "string representation (string_id),  \n",
    "the position of the start (start) and end of the match (end),  \n",
    "text in the corresponding span (span.text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76c9340a-26d3-4a18-907f-6d94f661f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_id: 476750250461617799\n",
      "string_id: my_matcher\n",
      "start: 151\n",
      "end: 154\n",
      "span.text: senses, particularly\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(f\"match_id: {match_id}\\r\\nstring_id: {string_id}\\r\\nstart: {start}\\r\\nend: {end}\\r\\nspan.text: {span.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5adb32-b65f-4e3a-ba00-1dae959cc0d8",
   "metadata": {},
   "source": [
    "## 3.6 GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6f6556b-7104-4b39-aff7-43bafd2d4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9c4e09a-1ab0-40bc-af3a-7c142e280e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1af8f0a9-3015-4be2-983d-fb05e4276dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sents = []\n",
    "for word in doc:\n",
    "    first_sents.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59a1fc48-fbdf-4624-8e56-0cdee5393730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A, wonderful, little, production, ., The, filming, technique, is, very, unassuming-, very, old, -, time, -, BBC, fashion, and, gives, a, comforting, ,, and, sometimes, discomforting, ,, sense, of, realism, to, the, entire, piece, ., The, actors, are, extremely, well, chosen-, Michael, Sheen, not, only, \", has, got, all, the, polari, \", but, he, has, all, the, voices, down, pat, too, !, You, can, truly, see, the, seamless, editing, guided, by, the, references, to, Williams, ', diary, entries, ,, not, only, is, it, well, worth, the, watching, but, it, is, a, terrificly, written, and, performed, piece, ., A, masterful, production, about, one, of, the, great, master, 's, of, comedy, and, his, life, ., The, realism, really, comes, home, with, the, little, things, :, the, fantasy, of, the, guard, which, ,, rather, than, use, the, traditional, ', dream, ', techniques, remains, solid, then, disappears, ., It, plays, on, our, knowledge, and, our, senses, ,, particularly, with, the, scenes, concerning, Orton, and, Halliwell, and, the, sets, (, particularly, of, their, flat, with, Halliwell, 's, murals, decorating, every, surface, ), are, terribly, well, done, .]\n"
     ]
    }
   ],
   "source": [
    "print(first_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caab55c0-dc18-4b71-bd9f-9e20959ddaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec([first_sents], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b45100e-3695-45c0-a1e2-5fccbecd7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec.my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1be18775-da8d-4117-af2b-054ee5a508d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[., seamless, see, truly, can, You, !, too, pat, down, voices, the, all, has, he, but, \", polari, the, all, got, the, editing, \", guided, it, but, watching, the, worth, well, it, is, only, not, ,, entries, diary, ', Williams, to, references, the, by, has, only, done, comforting, gives, and, fashion, BBC, -, time, -, old, very, unassuming-, very, is, technique, filming, The, ., production, little, wonderful, a, ,, not, and, Sheen, Michael, chosen-, well, extremely, are, actors, The, ., piece, entire, the, to, realism, of, sense, ,, discomforting, sometimes, is, a, terrificly, Orton, scenes, the, with, particularly, ,, senses, our, and, knowledge, our, on, plays, It, ., disappears, then, solid, remains, techniques, concerning, and, written, Halliwell, well, terribly, are, ), surface, every, decorating, murals, 's, Halliwell, with, flat, their, of, particularly, (, sets, the, and, ', dream, ', traditional, life, his, and, comedy, of, 's, master, great, the, of, one, about, production, masterful, A, ., piece, performed, and, ., The, realism, of, the, use, than, rather, ,, which, guard, the, fantasy, really, the, :, things, little, the, with, home, comes, A]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57a4ba6-f185-4f93-9578-0d0422e9b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model.wv['rather'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33855a2-4f1e-4128-a43c-af8258c8003c",
   "metadata": {},
   "source": [
    "## 3.7 NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b67b7-c2f6-487e-aa65-3629242cbc50",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) in the spaCy library is the process of extracting and classifying named entities from text. Named entities are specific entities such as persons, organizations, locations, dates, times, currencies, etc.\n",
    "\n",
    "NER in spaCy uses machine learning to recognize and classify named entities in text. The library provides pre-trained models that can be used to perform NER in different languages and domains.\n",
    "\n",
    "When the NER spaCy model is applied to text, it separates it into tokens (individual words or parts of words) and then determines whether each token is a named entity or not. If the token is classified as a named entity, the model also defines the type of that entity (for example, person, organization, location, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7f179-6d8c-4e1d-bdda-44d1df0a6ecf",
   "metadata": {},
   "source": [
    "Applying NER to text and extracting named entities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8260ae37-d86d-4b2c-bf33-e65f72f95cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('first', 'ORDINAL'), ('last night', 'TIME'), ('Australian', 'NORP'), ('Cut', 'PERSON'), ('Australia', 'GPE'), ('gore', 'PERSON'), ('Kylie Minogue', 'PERSON'), ('Molly Ringwald', 'PERSON'), ('Simon Bossell', 'PERSON'), ('Castle', 'ORG'), ('the last couple or years', 'DATE')]\n",
      "[('CIA', 'ORG'), ('Olivia', 'ORG'), ('Tom Conti', 'PERSON'), ('Hendrick Haese', 'PERSON'), ('Roger Moore', 'PERSON')]\n",
      "[('ITV', 'ORG'), ('last night', 'TIME'), ('Rupert Grint', 'PERSON'), ('Ben', 'PERSON'), ('Dame Eve Walton', 'PERSON'), ('Julie Walters', 'PERSON'), ('Driving Lessons', 'WORK_OF_ART'), ('2 hours', 'TIME')]\n",
      "[('two', 'CARDINAL'), ('Pepsi', 'ORG'), ('Kimberly Williams', 'PERSON'), ('Kimberly Williams', 'PERSON')]\n",
      "[('One', 'CARDINAL'), ('just 1 Oz', 'PERCENT'), ('the Oswald Maximum Security State Penitentary', 'ORG'), ('Emerald City', 'GPE'), ('Aryans', 'NORP'), ('Muslims', 'NORP'), ('Latinos', 'ORG'), ('Christians', 'NORP'), ('Italians', 'NORP'), ('Irish', 'NORP'), ('first', 'ORDINAL'), ('Watching Oz', 'PERSON')]\n",
      "[('Whoopi Goldberg', 'PERSON')]\n",
      "[('Merkerson', 'PERSON'), ('millions', 'CARDINAL'), ('Law', 'PERSON'), ('HBO', 'ORG'), ('Lackawanna Blues', 'PERSON'), ('Nanny', 'ORG'), ('Lackawanna', 'GPE'), ('New York', 'GPE'), ('Buffalo', 'GPE'), ('African-American', 'NORP'), ('50', 'CARDINAL'), ('Atlanta', 'GPE'), ('Sweet Auburn', 'GPE'), (\"New York's\", 'GPE'), ('Harlem', 'LOC'), ('Nanny', 'ORG'), ('Nanny', 'ORG'), ('Jimmy Smits', 'PERSON')]\n",
      "[('one', 'CARDINAL'), ('Donald Sutherland', 'PERSON'), ('Assasain', 'PERSON'), ('Tia Carerra', 'PERSON'), ('Thomas Ian Griffin', 'PERSON'), ('Max', 'PERSON'), ('DEA', 'ORG'), ('Diane', 'PERSON'), ('FBI', 'ORG'), ('Carerra', 'PERSON'), ('John Lithgow', 'PERSON'), ('Frazier on TV', 'ORG'), ('Livingston', 'PERSON'), ('Max', 'PERSON'), ('Diane', 'PERSON'), ('Livingston', 'PERSON'), ('the Assassain Sutherland', 'GPE'), ('Carerra', 'PERSON'), ('Griffith', 'PERSON'), ('Russian', 'NORP'), ('Italian Mafia', 'PERSON'), ('Chineese Mafia', 'PERSON'), ('Boston', 'GPE'), ('Sutherland', 'GPE'), ('10', 'CARDINAL')]\n",
      "[('second', 'ORDINAL'), ('second', 'ORDINAL'), ('first', 'ORDINAL'), ('Saboteur Hitchcock', 'PERSON'), ('one', 'CARDINAL'), ('Hitchcock', 'ORG'), ('States', 'GPE'), ('two', 'CARDINAL'), ('Otto Kruger', 'PERSON'), ('one', 'CARDINAL'), ('Hitchcock', 'ORG')]\n",
      "[('Sean Penn', 'PERSON'), ('Sarandon', 'PERSON'), ('Robbins', 'PERSON')]\n",
      "[('one', 'CARDINAL'), ('one', 'CARDINAL'), ('It Came From the Planet of Plot Contrivances', 'WORK_OF_ART'), ('MacGuffins', 'ORG'), ('a couple days', 'DATE'), ('BAM', 'ORG'), ('only a couple hours', 'TIME'), ('zero hour', 'TIME'), ('the last 10 minutes', 'TIME')]\n",
      "[('DR', 'GPE'), ('about 10 minutes', 'TIME'), ('105 minutes', 'TIME'), ('Hongkong', 'GPE'), ('RPG', 'ORG'), ('RPG', 'ORG'), ('RPG', 'ORG'), ('D&D', 'ORG'), ('Gamers', 'NORP'), ('DR', 'GPE'), ('RPG', 'ORG'), ('RPG', 'ORG')]\n",
      "[('less than one percent', 'PERCENT'), ('greater than 99%', 'PERCENT'), ('Relativity', 'ORG'), ('Quantum', 'ORG'), ('String', 'PERSON'), ('Mathematics', 'LOC'), ('three', 'CARDINAL'), ('the Hubble Telescope', 'FAC'), ('Science', 'ORG'), ('Philisophy', 'FAC'), ('one', 'CARDINAL'), ('Bravo Brian Greene', 'PERSON'), ('P.S.', 'GPE'), ('Universe', 'ORG'), ('Quantum', 'ORG'), ('Bleep', 'ORG'), ('Shaman', 'PERSON'), ('one', 'CARDINAL'), ('\"The Secret', 'WORK_OF_ART'), ('2006', 'DATE'), ('The Law of Attraction', 'WORK_OF_ART'), ('Universe', 'ORG'), ('Jesus/Mohammad/Buddha', 'PERSON'), ('today', 'DATE'), ('Texas', 'GPE'), ('Universe', 'ORG')]\n",
      "[('Jack Frost', 'PERSON'), ('First', 'ORDINAL'), ('Waaaaaaaaaaay', 'GPE'), ('Michael Keaton', 'PERSON')]\n",
      "[('Helena Bonham Carter', 'PERSON'), ('Kenneth Branagh', 'PERSON'), ('Bonham Carter', 'PERSON')]\n",
      "[('two', 'CARDINAL'), ('\\x84comedy', 'PERSON'), ('\\x96', 'PERSON'), ('Ernst Lubitsch', 'PERSON'), ('Billy Wilder', 'PERSON'), ('Mel Brooks', 'PERSON'), ('Dani Levy', 'PERSON'), ('two', 'CARDINAL'), ('Dani Levy', 'PERSON'), ('two', 'CARDINAL'), ('one', 'CARDINAL'), ('Jew', 'NORP'), ('West Germany', 'GPE'), ('one', 'CARDINAL'), ('third', 'ORDINAL'), ('Fast Eddie Felson', 'PERSON'), ('East Germany', 'GPE'), ('\\x96', 'PERSON'), ('Semel', 'PERSON'), ('Helmut Kohl', 'PERSON'), ('Orthodox', 'NORP'), ('Jew', 'NORP'), ('Levy', 'PERSON'), ('Hitler', 'PERSON'), ('Vai', 'PERSON')]\n",
      "[('the British Army', 'ORG'), ('carouse & brawl', 'ORG'), ('Imperial India', 'ORG'), ('Queen', 'PERSON'), ('Three', 'CARDINAL'), ('Kipling', 'GPE'), ('Hollywood', 'GPE'), ('Year of 1939', 'DATE'), ('suspense & humor', 'ORG'), ('three', 'CARDINAL'), ('Cary Grant', 'PERSON'), ('Victor McLaglen &', 'PERSON'), ('Douglas Fairbanks Jr.', 'PERSON'), ('thirds', 'CARDINAL'), ('time.(It', 'PERSON'), ('McLaglen', 'ORG'), ('World War One', 'EVENT'), ('World War Two', 'EVENT'), ('no fewer than 4', 'CARDINAL'), ('Grant', 'ORG'), ('British Intelligence', 'ORG'), ('Nazi', 'NORP'), ('Sam Jaffe', 'PERSON'), ('Kipling', 'GPE'), ('Montague Love', 'PERSON'), ('Eduardo Ciannelli', 'PERSON'), ('Evil Incarnate', 'PERSON'), ('Thuggee', 'LOC'), ('Joan Fontaine', 'PERSON'), ('Robert Coote', 'PERSON'), ('Lumsden Hare', 'PERSON'), ('Cecil Kellaway', 'GPE'), ('Miss Fontaine', 'PERSON'), ('Thuggee', 'LOC'), ('Kali', 'PERSON'), ('Hindu', 'NORP'), ('Indian', 'NORP'), ('6 centuries', 'DATE'), ('30,000', 'CARDINAL'), ('1840', 'DATE'), ('British', 'NORP'), ('novels & nightmares', 'ORG')]\n",
      "[('here)Stealing Sinatra', 'NORP'), ('half', 'CARDINAL'), ('about dimwit', 'QUANTITY'), ('dimwit', 'PERSON'), ('4/10', 'CARDINAL')]\n",
      "[('Down Periscope', 'WORK_OF_ART'), ('first', 'ORDINAL'), ('VHS', 'FAC'), ('Cinema Now', 'ORG'), ('first', 'ORDINAL'), ('the USS Drum', 'PRODUCT'), ('Mobile', 'GPE'), ('Alabama', 'GPE'), ('2002', 'DATE'), ('Cub Scouts', 'ORG'), ('Navy', 'ORG'), ('Hollywood', 'GPE'), ('Kelsey Grammar', 'PERSON'), ('Lauren Holly', 'PERSON'), ('Rob Schneider', 'PERSON')]\n",
      "[('Pam Dixon', 'PERSON'), ('Brenda Fricker', 'PERSON'), ('New York', 'GPE'), ('Ben Johnson', 'PERSON'), ('Adrien Brody', 'PERSON'), ('The Village', 'GPE'), ('Jay O. Sanders', 'PERSON'), ('JFK', 'PERSON'), ('The Day After Tomorrow).The', 'EVENT'), ('Joseph Gordon-Levitt', 'PERSON'), ('Lookout', 'ORG'), ('Sidekick Milton Davis Jr.', 'PERSON'), ('two', 'CARDINAL'), ('4', 'CARDINAL'), ('Emmy Nominee', 'PERSON'), ('Danny Glover', 'PERSON'), ('Lethal Weapon', 'ORG'), ('the day', 'DATE'), ('Angels', 'ORG'), ('George Knox', 'PERSON'), ('Roger', 'PERSON'), ('Angels', 'ORG'), ('3', 'CARDINAL'), ('Emmy', 'PERSON'), ('Christopher Lloyd', 'PERSON'), ('Al (Lloyd', 'ORG'), ('Roger', 'PERSON'), ('Dorothy Kingsley', 'PERSON'), (\"George Wells'\", 'PERSON'), ('GW Oscar Winner', 'PERSON'), ('1951', 'DATE'), ('William Dear', 'PERSON'), ('Directors Guild of America', 'ORG'), ('Mel Clark', 'PERSON'), ('Tony Danza', 'PERSON'), ('4', 'CARDINAL'), ('Golden Globe', 'FAC'), ('Emmy', 'NORP'), ('Anaheim', 'ORG'), ('Clark', 'PERSON'), ('Cinci', 'ORG'), ('Knox', 'PERSON'), ('the 18th century', 'DATE'), ('Matthew McConaughey', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(X_train['review'][0:20], disable=[\"tok2vec\", \"tagger\",  \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
